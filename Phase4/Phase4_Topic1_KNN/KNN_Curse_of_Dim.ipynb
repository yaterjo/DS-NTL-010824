{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f6e850a",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:#5642C5;\n",
    "           font-size:200%;\n",
    "           font-family:Arial;letter-spacing:0.5px\">\n",
    "\n",
    "<p width = 20%, style=\"padding: 10px;\n",
    "              color:white;\">\n",
    "K Nearest Neighbors and the Curse of Dimensionality\n",
    "              \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "DS-NTL-010824\n",
    "<p>Phase 4</p>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align = \"right\">\n",
    "<img src=\"Images/flatiron-school-logo.png\" align = \"right\" width=\"200\"/>\n",
    "</div>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ead8d00",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Objectives"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a742ae3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Describe the $k$-nearest neighbors algorithm\n",
    "- Identify multiple common distance metrics\n",
    "- Tune $k$ appropriately in response to models with high bias or variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9138f536",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d395f711",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src = \"Images/neighbor_tyranny.jpg\" width = 450/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00260809",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>There's truth here. But we can also rely on this tyranny to help us build classifiers.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f92dbfd3",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center> Let's do it. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f3d60b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### K-nearest neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f4de51",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Process of predicting in k-nearest algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a7fd7f",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Take a **test** data point, look at it's $k$ nearest neighbors in **training** set:\n",
    "\n",
    "- Look at what class neighbors in training set all are.\n",
    "- Count class tallies all up.\n",
    "- Assign datapoint to winning majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03756b45",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src = \"Images/knn_3.webp\" width = 800/></center>\n",
    "<center> k = 3 neighbors </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4aa2ce",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Concept is easy. \n",
    "- Doing this efficiently: exercise in algorithms and data structures.\n",
    "\n",
    "\n",
    "More details:\n",
    "\n",
    "https://towardsdatascience.com/tree-algorithms-explained-ball-tree-algorithm-vs-kd-tree-vs-brute-force-9746debcd940\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c0c50a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Process of training k-nearest algorithm**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2332aa",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Store the training data points.\n",
    "- Yeah thats it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48fb1181",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### The role of $k$: number of neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d48bef6",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<img src = \"Images/KNN_nearestnn.png\" width = 400 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4f0ab5",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Changing neighbors:\n",
    "- Clearly affects which KNN predicts for test point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eab9f8",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Lower $k$ implies higher sensitivity to fluctuations (higher variance)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f776ddac",
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "**Low $k$ = higher variance**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5121fb0e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./images/knn-process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a14d917",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "If $k$ increases too much:\n",
    "- Always predicts majority class in training set.\n",
    "- Model too \"rigid\"\n",
    "- Not sensitive enough to relationship between target and features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f9a570",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/knn_neighbor_increase.jpeg\" width = 600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6fde42c",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Too high $k$ means high bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfb4ab5",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In summary:\n",
    "<center><img src = \"Images/bias_variance.png\" /></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef20141b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Before we even think about running this or any distance-based model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35780fda",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center>SCALE THE FEATURES!!!</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0572d3c",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Otherwise: \n",
    "- Weights distances in different feature dimensions differently"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc8fe855",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src = \"Images/nonnormal.png\" /></center>\n",
    "<center>Unscaled</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1582b1e5",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<center><img src = \"Images/normalized.png\" /></center>\n",
    "<center>Scaled</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1418da2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Run KNN classifier on a dataset and tune hyperparameters:\n",
    "- Load that iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03291fb",
   "metadata": {
    "cell_style": "center",
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "iris_df = pd.read_csv('Data/Iris.csv').drop(columns = ['Id'])\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f40ba0",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "target_transform = LabelEncoder()\n",
    "iris_df['Species'] = target_transform.fit_transform(iris_df['Species'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb5c8aa",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "iris_df['Species']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe5eef5d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- What next steps should we take?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42500119",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Visualize the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892121cd",
   "metadata": {
    "cell_style": "center",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(hue = 'Species', data = iris_df, height = 1.75)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b54eccad",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For instruction sake let's do classification with following two features:\n",
    "- sepal width\n",
    "- petal width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5091883",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(y = 'SepalWidthCm', x = 'PetalWidthCm', hue = 'Species', data = iris_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64a4c5ee",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Some overlap between class 1 and 2:\n",
    "- Virginica\n",
    "- Versicolor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811134e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "What next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2f7ac39",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "1. Separate dataframe into X (features) and y (labels)\n",
    "2. Train/test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2673b20",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# so whats next?\n",
    "X = iris_df[['SepalWidthCm', 'PetalWidthCm']]\n",
    "y = iris_df['Species']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd60505",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# shuffle and split, stratify keeps target distribution same in train/test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.15, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3660a752",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we standardize features, right?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654109ff",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Depends!\n",
    "- If doing simple train/test split, then yes!\n",
    "- If doing cross-validation, then no!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74abad8a",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Mess around with $k$:\n",
    "- Look at performance\n",
    "- Hyperparameter tune/cross-validate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa59701",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "So we won't fit the standardizer on the entire training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "434563a2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4390f8a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fold_index = KFold(n_splits = 5).split(X_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4023ab0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "next(fold_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed132bb",
   "metadata": {
    "cell_style": "center",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "def cross_validation(X_train, y_train, k, num_split = 10):\n",
    "    \n",
    "    X_train = X_train.values\n",
    "    y_train = y_train.values\n",
    "    \n",
    "    score_train_list = []\n",
    "    score_val_list = []\n",
    "    \n",
    "    for train_index, valid_index in KFold(n_splits = num_split).split(X_train):\n",
    "\n",
    "\n",
    "        # train and validation splitting \n",
    "        X_train_fold, X_val_fold = X_train[train_index], X_train[valid_index]\n",
    "        y_train_fold, y_val_fold = y_train[train_index], y_train[valid_index]\n",
    "\n",
    "        #create/fit the Standard scaler on the train fold\n",
    "        scaler = StandardScaler()\n",
    "        X_tf_sc = scaler.fit_transform(X_train_fold)\n",
    "        # transform validation fold\n",
    "        X_vld_sc = scaler.transform(X_val_fold)\n",
    "\n",
    "        # create/fit knearest neighbor\n",
    "        knn = KNeighborsClassifier(n_neighbors = k)\n",
    "        knn.fit(X_tf_sc, y_train_fold)\n",
    "        \n",
    "        # now how did we do?\n",
    "        accuracy_train = knn.score(X_tf_sc, y_train_fold)\n",
    "        accuracy_val = knn.score(X_vld_sc, y_val_fold)\n",
    "        score_val_list.append(accuracy_val)\n",
    "        score_train_list.append(accuracy_train)\n",
    "    \n",
    "    return {'k': k, 'train': np.mean(score_train_list), 'validation': np.mean(score_val_list)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2259ddd",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# empty dataframe\n",
    "crossval_df = pd.DataFrame(columns = ['k', 'train', 'validation'])\n",
    "\n",
    "# append results for each value of k\n",
    "for k in np.arange(1,100):\n",
    "\n",
    "                            \n",
    "    c = cross_validation(X_train, y_train, k, 5)\n",
    "    c_df = pd.DataFrame(c,index=[0])\n",
    "    crossval_df = pd.concat([crossval_df,c_df]) \n",
    "    \n",
    "\n",
    "\n",
    "crossval_df.reset_index(drop = True, inplace = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d746a71a",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "crossval_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1bba57",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(x = 'k', y = 'train', \n",
    "             data = crossval_df, \n",
    "             ax = ax, label = 'train')\n",
    "sns.lineplot(x = 'k', y = 'validation', \n",
    "             data = crossval_df,\n",
    "             ax = ax, label = 'validation')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('5-fold CV tuning KNN')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5297264d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Some bias-variance tradeoff in action as a function of $k$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfc0083",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# let's find our best performing k-value\n",
    "crossval_df.iloc[crossval_df['validation'].idxmax()]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7305a1",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "So we determined best estimator at $k = 4$:\n",
    "- Fit the full train set with estimator at this value of $k$.\n",
    "- Evaluate performance on true test/hold-out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b10f7b8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "fulltrain_scaler = StandardScaler()\n",
    "X_train_sc = fulltrain_scaler.fit_transform(X_train)\n",
    "X_test_sc = fulltrain_scaler.transform(X_test)\n",
    "\n",
    "best_estimator = KNeighborsClassifier(n_neighbors = 4)\n",
    "best_estimator.fit(X_train_sc, y_train)\n",
    "\n",
    "# get predictions\n",
    "y_pred = best_estimator.predict(X_test_sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d70b2fb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "#from sklearn.metrics import plot_confusion_matrix\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77efc41a",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#plot_confusion_matrix(best_estimator, X_test_sc, y_test);\n",
    "ConfusionMatrixDisplay.from_estimator(best_estimator, X_test_sc, y_test);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1167306f",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0474b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In 2D feature space: \n",
    "- visualize decision boundary\n",
    "- prediction space vs. actual data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b4ca15",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X_full_sc = fulltrain_scaler.transform(X)\n",
    "x_min, x_max = X_full_sc[:, 0].min() - 1, X_full_sc[:, 0].max() + 1\n",
    "y_min, y_max = X_full_sc[:, 1].min() - 1, X_full_sc[:, 1].max() + 1\n",
    "xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.1), np.arange(y_min, y_max, 0.1))\n",
    "\n",
    "f, ax = plt.subplots()\n",
    "\n",
    "Z = best_estimator.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "Z = Z.reshape(xx.shape)\n",
    "ax.contourf(xx, yy, Z, alpha=0.4)\n",
    "ax.scatter(X_full_sc[:, 0], X_full_sc[:, 1], c = y, s=30, edgecolor=\"k\")\n",
    "ax.set_xlabel('Sepal width [scaled]')\n",
    "ax.set_ylabel('Petal width [scaled]')\n",
    "ax.set_title('Decision Boundary: KNN (k = 4)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ec2c592",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This is pretty decent and not too bad on the overfitting:\n",
    "- Given small dataset size\n",
    "- Some irregularities in decision boundary \n",
    "- Validation score as a function of $k$\n",
    "    - Consider increasing $k$ a little."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e2719b3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### The effectiveness of the KNN classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0804c2fb",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Naturally learns complex boundaries.\n",
    "- KNN can be prone to overfitting\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a17ab9a",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Another less obvious point:**\n",
    "- Performance scales badly as the number of features get big\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a926740d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Let's see why"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b51dae",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Take points in 2D:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a297bb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/1dto2D_CoD.gif\" width = 500/>\n",
    "<center> Same number of points denser in 1D. </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9786895b",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Extend 2D to 3D:\n",
    "- Number of points fixed\n",
    "- Random z-position selection within the cube\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb681a25",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/2dto3d_CoD.gif\" width = 500/>\n",
    "<center> The density now?</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9ed0a1",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Extend 3D to 4D (add time dimension):\n",
    "- Snapshots = 3D cross-section of 4D-cube.\n",
    "\n",
    "Visualize from 1D to 4D."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9188649a",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/CoD.gif\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaa88cf",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "As my dimensionality increases:\n",
    "\n",
    "- Neighbors get sparse.\n",
    "- volume scales **exponentially** with number of features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ab2575",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Being more systematic:\n",
    "\n",
    "Volume of sphere in $M$ dimensions:\n",
    "\n",
    "$$ V_M(R) = \\frac{\\pi^{M/2}}{\\Gamma(\\frac{M}{2} + 1)}R^M $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b7a3b90",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from scipy.special import gamma\n",
    "\n",
    "def sphere_calc(m, R):\n",
    "    numerator = (R**m * np.pi**(m/2))\n",
    "    denom = gamma(m/2 + 1)\n",
    "    \n",
    "    return numerator/denom\n",
    "\n",
    "dim_list = pd.Series(np.arange(1,21))\n",
    "sph_vol_list = dim_list.map(lambda M: sphere_calc(M, 2)) # get +- 2 std of standardized variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5501dfe2",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now if I have moderate sized dataset of $N = 5000$ points:\n",
    "- After variable standardization: ~ $\\pm 2 \\sigma$\n",
    "- Sphere of radius 2.\n",
    "- Let's see how the density scales: $$ \\rho = \\frac{N}{V_M(2)} $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ae2626",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "density_vs_dim = 5000/sph_vol_list\n",
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(x = density_vs_dim.index, y = density_vs_dim.values, ax = ax)\n",
    "ax.set_ylabel('Point density')\n",
    "ax.set_xlabel('Number of Features')\n",
    "ax.set_title('Point density:  feature dimension scaling')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d8aa6c",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Life is a lonely place at high dimension:\n",
    "- Unlikely to have a neighbor within a unit sphere at moderate to high dimensions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08fb6ba5",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "density_vs_dim.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd8458d",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "This affects all algorithms in small to mid-size problems:\n",
    "- Need good statistical sampling in feature space for training\n",
    "- Dataset become sparse in high dimension. Hard to do statistical learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e70afc0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Local, distance based algorithms are especially affected by high D**\n",
    "- Classification built off point-by-point local consideration.\n",
    "- Nearest neighbors of a given point: very far away in feature space.\n",
    "- For each point: unreasonable to rely on tyranny of neighbors.\n",
    "\n",
    "Majority voting with KNN: starts to be a bad idea."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7868bd",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Solution**: Dimensionality reduction techniques (will talk about later)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "288e474e",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Level Up: Distance Metrics\n",
    "> The \"closeness\" of data points → proxy for similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f31e829",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](./images/distance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a596abe",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Minkowski: $dist(A,B) = (\\sum_{k=1}^{N} |a_k - b_k|^c)^\\frac{1}{c} $$\n",
    "\n",
    "\n",
    "\n",
    "- Manhattan: $dist(A,B) = \\sum_{k=1}^{N} |a_k - b_k|$\n",
    "\n",
    "\n",
    "\n",
    "- Euclidean: $dist(A,B) = \\sqrt{ \\sum_{k=1}^{N} (a_k - b_k)^2 }$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5e5b23e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "There are quite a few different distance metrics built-in for Scikit-learn: \n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.metrics.DistanceMetric.html?highlight=distance#sklearn.metrics.DistanceMetric"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7ebdd9",
   "metadata": {},
   "source": [
    "Pros and cons of distance metrics:\n",
    "\n",
    "https://towardsdatascience.com/9-distance-measures-in-data-science-918109d069fa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4372ac78",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dplearn",
   "language": "python",
   "name": "dplearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
