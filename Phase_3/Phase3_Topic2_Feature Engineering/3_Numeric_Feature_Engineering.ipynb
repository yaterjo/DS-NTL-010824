{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a7dc42c",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:#5642C5;\n",
    "           font-size:200%;\n",
    "           font-family:Arial;letter-spacing:0.5px\">\n",
    "\n",
    "<p width = 20%, style=\"padding: 10px;\n",
    "              color:white;\">\n",
    "Feature Engineering: Scaling, Polynomials, and Interactions\n",
    "              \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "DS-NTL-010824\n",
    "<p>Phase 3</p>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align = \"right\">\n",
    "<img src=\"Images/flatiron-school-logo.png\" align = \"right\" width=\"200\"/>\n",
    "</div>\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca486440",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Feature Engineering: Transforming input data\n",
    "- Create new features for use in modeling\n",
    "- Input in form that the better conforms to structure of input-output relationship.\n",
    "- Model interactions between features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aacd2a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.stats import norm\n",
    "noise = norm.rvs(scale = 16, size = 400)\n",
    "x = np.linspace(-7,7, 400) + 300\n",
    "y = 3*(x-300)**2  + 4*(x-300) + 100 + noise\n",
    "tempyield_data = pd.DataFrame({'Temp': x, 'Yield': y})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766c3371",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Target may not depend linearly on feature set.\n",
    "- I.e. cant approximate well by a hyperplane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb2bba0",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%capture yieldvtemp\n",
    "sns.set_context('talk')\n",
    "fig, ax = plt.subplots()\n",
    "sns.scatterplot(y ='Yield', x = 'Temp', data = tempyield_data, ax = ax)\n",
    "ax.set_title('Yield vs. Temperature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f393236b",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "yieldvtemp()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87173b27",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Fitting yield to a model simply linear in temperature might be considered silly. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8540e3cf",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "What kind of dependence might we try?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a0b3f6",
   "metadata": {
    "cell_style": "split",
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tempyield_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2800b3d8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Perform a regression:\n",
    "$$ Y = \\beta_2T^2 + \\beta_1T + \\beta_0  $$\n",
    "\n",
    "*Note*: this is still a model **linear** in coefficients. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af72fb20",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Construct another column that is $T^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8719c2",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tempyield_data['Temp_squared'] = tempyield_data['Temp']**2\n",
    "tempyield_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9d5c5b2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Linear regression of Yield on $T$ and $T^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c241fd26",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Doing a regression just on T. Silly but lets do it anyway to see:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84965f17",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac5ec5d",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = tempyield_data['Temp']\n",
    "y = tempyield_data['Yield']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42)\n",
    "\n",
    "lr_simple = LinearRegression()\n",
    "lr_simple.fit(X_train.values.reshape(-1,1), y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7794d95",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(lr_simple.coef_, lr_simple.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac6d493",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lr_simple.score(X_train.values.reshape(-1,1),\n",
    "                y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b52ddf",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = lr_simple.predict(X_test.values.reshape(-1,1))\n",
    "mean_absolute_error(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e981fcc8",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "But saying there is no relation between temperature and yield is obviously a mistake!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3310434",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Regression on $T$ and $T^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee27eec8",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_poly = tempyield_data[['Temp', 'Temp_squared']]\n",
    "y = tempyield_data['Yield']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_poly, y, test_size = 0.3)\n",
    "\n",
    "lr_poly = LinearRegression()\n",
    "lr_poly.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dddd5ebb",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(lr_poly.coef_, lr_poly.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0724dfe2",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lr_poly.score(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "237396d4",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_poly = lr_poly.predict(X_test)\n",
    "mean_absolute_error(y_pred_poly, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79532639",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's visualize the difference in predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7964ab0",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "beta_1, beta_2 = lr_poly.coef_\n",
    "beta_0 = lr_poly.intercept_\n",
    "\n",
    "poly_pred = beta_2*X**2 + beta_1*X + beta_0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X, poly_pred, c ='r')\n",
    "ax.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a45c4d",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "beta_1 = lr_simple.coef_[0]\n",
    "beta_0 = lr_simple.intercept_\n",
    "\n",
    "lin_pred = beta_1*X + beta_0\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(X, lin_pred, c ='r')\n",
    "ax.scatter(X, y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e70d1ec",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### More generally\n",
    "\n",
    "Target depend on feature with higher order polynomial. Procedure would be same.\n",
    "\n",
    "$$ Y = \\beta_0 + \\beta_1 x + \\beta_2 x^2 + ... + \\beta_n x^n $$\n",
    "\n",
    "Again:\n",
    "- Construct feature columns for each power of x.\n",
    "- Fit $\\beta_i$ using linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac9fc51",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Using scikit-learn: PolynomialFeatures with a single feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3ee25cb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- With single feature is easy/fast way to construct these features at higher power.\n",
    "- Again, main advantage: can be used in a pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed7b9cf0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42473d7",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "PolynomialFeatures()\n",
    "- Takes arguments for highest order of polynomial\n",
    "- Can chose to include zeroth power term for fitting bias (useful for statsmodels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c27b3617",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pf = PolynomialFeatures(degree = 2, include_bias = False )\n",
    "X_to_second = pf.fit_transform(X.values.reshape(-1,1))\n",
    "X_to_second[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c9f4cc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "And fitting is exactly the same as before."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd7978d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### PolynomialFeatures() with multiple features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15244f88",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sales_df = pd.read_csv('Data/advertising.csv').drop(columns = ['Unnamed: 0'])\n",
    "sales_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f66b914",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sales_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8748ade4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ax = sns.pairplot(sales_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31df3844",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "sns.heatmap(sales_df.corr(), annot = True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "016047c5",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Maybe want to model interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3fdf162",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Recall that $$ Cov(Radio, TV) \\rightarrow Radio\\times TV $$\n",
    "\n",
    "Average of product encodes correlation between the two."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "975a9a87",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Using products of features as new features:\n",
    "\n",
    "- Factors pairwise correlations of original features into prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93783625",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Model with linear terms and interactions:\n",
    "$$ Sales = \\beta_{TV} TV + \\beta_{Radio} Radio + \\beta_{Newspaper} Newspaper + \\\\ \\beta^{int}_1 \\Big( TV \\times Radio \\Big) + \\beta^{int}_2 \\Big( TV \\times Newspaper \\Big) + \\beta^{int}_3 \\Big(Radio \\times Newspaper\\Big) + \\beta_0 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "719a4f1b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "vs. model with just linear terms\n",
    "\n",
    "$$ Sales = \\beta_{TV} TV + \\beta_{Radio} Radio + \\beta_{Newspaper} Newspaper + \\beta_0 $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213697b7",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = sales_df[['TV', 'radio', 'newspaper']]\n",
    "y = sales_df['sales']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aab1aa9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "First conduct linear regression on the raw features:\n",
    "- train test split\n",
    "- then perform standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b2a7f4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 42\n",
    "                                                   )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0db65ce",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We could apply pandas .apply():\n",
    "- very inconvenient when doing train test splits\n",
    "- fitting parameters on train set\n",
    "- applying trasformation based on those parameters to:\n",
    "    - train\n",
    "    - test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5647a7eb",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Scikit-learn has transformer object that standardizes columns:\n",
    "- just for this purpose on train/test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d79c41",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "ss = StandardScaler()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d619d540",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now extract means and standard deviations for each column:\n",
    "- .fit() method\n",
    "Then apply transformartion to both train and test:\n",
    "- .transform() method\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6aa68a6",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ss.fit(X_train)\n",
    "X_standardized_train = ss.transform(X_train)\n",
    "X_standardized_test = ss.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6eed86",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_standardized_train.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4f627f7",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_standardized_train.std(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3419258d",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_standardized_test.mean(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3e3cf0",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "X_standardized_test.std(axis = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18344068",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now define and fit the model on the train set using the standardized training features:\n",
    "- get $R^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f29d337",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lr_raw = LinearRegression()\n",
    "lr_raw.fit(X_standardized_train, y_train)\n",
    "lr_raw.score(X_standardized_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3015ee31",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(lr_raw.coef_,\n",
    "          index = X.columns) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e991c7f3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This jives with our expectations from EDA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edad0a5e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Evaluating on the test set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ad6223",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = lr_raw.predict(X_standardized_test)\n",
    "mean_absolute_error(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d28122b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Can we do better with Polynomial Features?\n",
    "- Let's include the interaction terms!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db36e91",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "PolynomialFeatures:\n",
    "- has argument interaction_only = True/False.\n",
    "- If true only takes interactions terms at highest order specified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c3e0a7",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "linear_withinteraction = PolynomialFeatures(interaction_only = True, include_bias = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a623d60d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We've already made the train/test split:\n",
    "- X_train, X_test, y_train, y_test\n",
    "\n",
    "Fit PolynomialFeatures to train set, then transform both sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488b22fa",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#fit\n",
    "linear_withinteraction.fit(X_train)\n",
    "\n",
    "# get the feature names\n",
    "feat_names = pd.Series(linear_withinteraction.get_feature_names_out())\n",
    "feat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c40a96",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "Xpolytrans_train = linear_withinteraction.transform(X_train)\n",
    "Xpolytrans_test = linear_withinteraction.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68046ae3",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "After pandafying our numpy array for train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86505e88",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "#run code only if you use 'get_feature_names()' in the code block above\n",
    "map_dict = {'x0': 'TV', 'x1': 'Radio', 'x2': 'Newspaper'}\n",
    "for key, value in map_dict.items():\n",
    "    feat_names = feat_names.str.replace(key, value)\n",
    "X_trans_df = pd.DataFrame(Xpolytrans_train, columns = feat_names)\n",
    "X_trans_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0061bc55",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# create dataframe of the interactions\n",
    "X_trans_df = pd.DataFrame(Xpolytrans_train ,columns = feat_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daa06b71",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_trans_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2585dc1d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Looks good. Let's standardize the variables and analyze the weights of the factors and their interactions.\n",
    "- Again: apply the standardscaler to the train set with interaction and transform test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a352bf",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# standardizes each column in feature matrix. Scikit-learn has a transformer for this.\n",
    "\n",
    "# object does the standardization\n",
    "ss = StandardScaler() \n",
    "\n",
    "# fit and transforms new polynomial feature matrix\n",
    "X_trans_train = ss.fit_transform(Xpolytrans_train) \n",
    "\n",
    "# uses fit parameters from train set to standardize test set as well\n",
    "X_trans_test = ss.transform(Xpolytrans_test) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d90116",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Getting $R^2$ and coefficients for our standardized linear model with interactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba903a0",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lr_poly_scaled = LinearRegression()\n",
    "lr_poly_scaled.fit(X_trans_train, y_train)\n",
    "\n",
    "# gets R^2 value for train set\n",
    "lr_poly_scaled.score(X_trans_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9823307c",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pd.Series(lr_poly_scaled.coef_,\n",
    "          index = X_trans_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce57aa0b",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "y_pred_interactions = lr_poly_scaled.predict(X_trans_test)\n",
    "y_pred_interactions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a65f18",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A comparison on test error with a mode linear in the features vs including interaction terms:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7ff68a",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mean_absolute_error(y_pred_interactions, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e2998f",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The old model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d62ada",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mean_absolute_error(y_pred, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297e34a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Some words of advice:\n",
    "- Adding interactions and higher order terms can be useful.\n",
    "- But they can also add extra variables that can lead to overfitting or useless parameters.\n",
    "- Be careful and use your judgment.\n",
    "\n",
    "#### Also: interaction terms not always interpretable. But may lead to better predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6710c1b9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Takeaway:\n",
    "- This is where EDA is really important:\n",
    "    - Visualization\n",
    "    - Descriptive statistics \n",
    "    - Correlation matrices\n",
    "    - Playing with your data!\n",
    "    \n",
    "See what transformations and functional relations useful for model inference and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26e07102",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a81e88f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dplearn",
   "language": "python",
   "name": "dplearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
