{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "79adf0f5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div style=\"color:white;\n",
    "           display:fill;\n",
    "           border-radius:5px;\n",
    "           background-color:#5642C5;\n",
    "           font-size:200%;\n",
    "           font-\\amily:Arial;letter-spacing:0.5px\">\n",
    "\n",
    "<p width = 20%, style=\"padding: 10px;\n",
    "              color:white;\">\n",
    "Regularization\n",
    "              \n",
    "</p>\n",
    "</div>\n",
    "\n",
    "DS-NTL-010824\n",
    "<p>Phase 3</p>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<div align = \"right\">\n",
    "<img src=\"Images/flatiron-school-logo.png\" align = \"right\" width=\"200\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1dfee06",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Objectives\n",
    "- Use the algorithm of cross-validation (with `sklearn`)\n",
    "- Explain the concept of regularization\n",
    "- Use Lasso and Ridge regularization in model design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8651f222",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58396d23",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Preventing Overfitting - Regularization\n",
    "Again, complex models are very flexible in the patterns that they can model but this also means that they can easily find patterns that are simply statistical flukes of one particular dataset rather than patterns reflective of the underlying data-generating process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2eb4ac3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "When a model has large weights, the model is \"too confident\". This translates to a model with high variance which puts it in danger of overfitting!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75efc955",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](images/punishing_model_metaphor.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb75f254",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We need to punish large (confident) weights by contributing them to the error function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f8391e3",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Some Types of Regularization:**\n",
    "\n",
    "1. Reducing the number of features\n",
    "2. Increasing the amount of data\n",
    "3. Popular techniques: Ridge, Lasso, Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39232bd9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Regularization for Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05c5d0a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Modify our squared error loss function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96060411",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ L = |\\textbf{y} - X \\textbf{w}|_2^2 + \\lambda |\\textbf{w}|_2^2 $$ \n",
    "\n",
    "with $|\\textbf{w}|_2^2 = w_1^2 + w_2^2 + ... + w_m^2$ as sum of squares of the feature weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbdcd374",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = \"Images/ridge_regression_geometric.png\" width = 450>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f5cdd1",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Tug of war between:\n",
    "\n",
    "Ridge cost: $ \\lambda |\\textbf{w}|_2^2 = \\lambda (w_1^2 + w_2^2)  $\n",
    "- L2 Regularization (Euclidean distance)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ec07e0",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Least squares cost: $ |\\textbf{y} - X\\textbf{w}|_2^2 $\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9fae68",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = \"Images/ridge_regression_geometric.png\" width = 450>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "839b2dae",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Regularization term Penalizes large weights: \n",
    "- **strongly** discourages large fluctuations in $\\textbf{w}$ depending on training set.\n",
    "- i.e. reduces Var[$\\textbf{w}$]\n",
    "- **Can lead to large performance boost on unseen data.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89552a9f",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- the tradeoff: repeating fitting on large number of distinct training sets:\n",
    "    - Average of $\\textbf{w}$ is off from best fit to population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6c9c007",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Our goal is to construct a model with:\n",
    "- as low a bias as possible (gets close to the true $\\textbf{w}$ if we had/fit on the entire population)\n",
    "- as low a *model* variance as possible (spread in $\\textbf{w}$ is low):\n",
    "    - implies $\\textbf{w}$ is tightly clustered\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10570897",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Idea of Ridge: \n",
    "- Tune $\\lambda$ just right. This is something we input as external parameter to model. **Hyperparameter** \n",
    "- Cluster of $\\textbf{w}$ little off the center of the bullseye\n",
    "- But: tightly clustered."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24a7db96",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "With $\\lambda$ tuned well:\n",
    "\n",
    "- not likely to make generalization errors due to large fluctuation in $\\textbf{w}$\n",
    "- But doesnt shift $\\textbf{w}$ too far from least squares estimate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95ee25e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "OK let's do a Ridge regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbca8ef1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# model validation: testing model variance with cross validation\n",
    "\n",
    "from sklearn.model_selection import cross_val_score, cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82e47efe",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "birds = sns.load_dataset('penguins')\n",
    "birds = birds.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df85aabe",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "birds.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a134327",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Going to use the other features to predict the body mass of a penguin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e099fd4d",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X = birds.drop('body_mass_g', axis=1)\n",
    "y = birds['body_mass_g']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f02e781c",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X ,y, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83127842",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's one-hot encode the nominal categoricals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbd4e29a",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Taking in other features (category)\n",
    "ohe = OneHotEncoder(drop='first',handle_unknown= 'ignore')\n",
    "dummies = ohe.fit_transform(X_train[['species', 'island', 'sex']])\n",
    "\n",
    "# Getting a DF\n",
    "X_train_onehot = pd.DataFrame(dummies.todense(), columns=ohe.get_feature_names_out(), index=X_train.index)\n",
    "\n",
    "X_train_onehot.head()\n",
    "#dummies.todense()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9072c0e6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Attach this to dataframe with numerical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f8ee37",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train_numeric = X_train[['bill_length_mm', 'bill_depth_mm', 'flipper_length_mm']]\n",
    "X_train_df = pd.concat([X_train_numeric, X_train_onehot], axis=1)\n",
    "X_train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bbd822a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We have our training feature matrix:\n",
    "- Apply transformation fit_transformed on train set to test feature matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79229562",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_dummies = ohe.transform(X_test[['species', 'island', 'sex']])\n",
    "test_df = pd.DataFrame(test_dummies.todense(), columns=ohe.get_feature_names_out(),\n",
    "                       index=X_test.index)\n",
    "X_test_df = pd.concat([X_test[['bill_length_mm', 'bill_depth_mm',\n",
    "                              'flipper_length_mm']], test_df], axis=1)\n",
    "X_test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a171ff",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lr1 = LinearRegression()\n",
    "lr1.fit(X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74b82f9b",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lr1.score(X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bec397",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "y_pred = lr1.predict(X_test_df)\n",
    "np.sqrt(mean_squared_error(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03424149",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Wow that's a good $R^2$ value!\n",
    "\n",
    "- Estimate how we are doing on unseen data with cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b967516d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cv_results = cross_validate(X=X_train_df, y=y_train, estimator=lr1, cv=10, scoring=('r2', 'neg_mean_squared_error'),\n",
    "                return_train_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983762c4",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How well model explains training fold data in each iteration cross validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a12213a",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "train_res = cv_results['train_r2']\n",
    "train_res"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2d2788",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "It's fitting well each time in the 10 iterations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9613defb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How does the validation look?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e29199a",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_res = cv_results['test_r2']\n",
    "test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17733a51",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_res.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612080c5",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "test_res.std(ddof = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a86b87c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Good average test performance and relatively low variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2038f5d",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_train = np.sqrt(np.abs(cv_results['train_neg_mean_squared_error']))\n",
    "RMSE_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62a855a",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_train.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2dcbee8",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_train.std(ddof = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32d9d5c",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_test = np.sqrt(np.abs(cv_results['test_neg_mean_squared_error']))\n",
    "RMSE_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695703cf",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Clearly larger average RMSE and variance of RMSE in the test set. But still pretty decent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbebd585",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_test.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac50032",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_test.std(ddof = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "758cfd3e",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But...I'm a greedy man.\n",
    "\n",
    "I want to do better than this. \n",
    "- I'm going to add some polynomials to get a more complex model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bffb6ed8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "My motivation:\n",
    "    \n",
    "- More complex model = better able to capture more complex relationships between mass and other variables.\n",
    "- Better prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab31c497",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Adding model complexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b555934",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c2b759",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pf = PolynomialFeatures(degree=3)\n",
    "X_poly_train = pd.DataFrame(pf.fit_transform(X_train_df))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc2492b8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_poly_test = pf.transform(X_test_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfdce38e",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b6ac70f",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X_poly_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e56ebaa",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "poly_lr = LinearRegression()\n",
    "poly_lr.fit(X_poly_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4423ed",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "poly_lr.score(X_poly_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f53de1",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lr1.score(X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b99352e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "About a 3% improvement: \n",
    "- that could mean money in other contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b6b2724",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "poly_cv_results = cross_validate(\n",
    "                X=X_poly_train, \n",
    "                y=y_train,\n",
    "                estimator=poly_lr, \n",
    "                cv=10,\n",
    "                scoring=('r2', 'neg_mean_squared_error'),\n",
    "                return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e3720c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "poly_train_res = poly_cv_results['train_r2']\n",
    "poly_train_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fde2e53a",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "poly_train_res.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e346fc76",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "poly_train_res.std(ddof =1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99963559",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Wow...I'm ready to brag to my boss.\n",
    "\n",
    "- But let's check the performance on the validation folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513a58fa",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "poly_valid_res = poly_cv_results['test_r2']\n",
    "poly_valid_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64beacb9",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "poly_valid_res.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4bb5c4",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "poly_valid_res.std(ddof = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265c2d92",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_polytest = np.sqrt(np.abs(poly_cv_results['test_neg_mean_squared_error']))\n",
    "RMSE_polytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44768992",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_polytest.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e35df06",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_polytest.std(ddof=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2e7e28",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<center><img src = \"Images/punch_chuck_norris.gif\" width = 400/></center>\n",
    "<center>You just got punched in the face by the bias-variance problem.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b269524",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Fit an overly complex model:\n",
    "- Doesn't generalize well."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fd08e9d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's try regularizing polynomial model:\n",
    "$$ L = |\\textbf{y} - X \\textbf{w}|_2^2 + \\lambda |\\textbf{w}|_2^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1c7cf3",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "- reduce the floppiness/complexity of model\n",
    "- but still keep *some* of the complexity added by these polynomial features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbec4ee4",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Reduce Var[$\\textbf{w}$].\n",
    "- Get model predictions more representative of population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641b7505",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186734db",
   "metadata": {
    "hidden": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ss = StandardScaler()\n",
    "pf = PolynomialFeatures(degree=3)\n",
    "\n",
    "# You should always be sure to _standardize_ your data before\n",
    "# applying regularization!\n",
    "\n",
    "X_train_processed = pf.fit_transform(ss.fit_transform(X_train_df))\n",
    "X_test_processed = pf.transform(ss.transform(X_test_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2265328c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Absolutely need to standardize/normalize features:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29a6111c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " $$ L = |\\textbf{y} - X \\textbf{w}|_2^2 + \\lambda |\\textbf{w}|_2^2 $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14509987",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "L2 regularization cost function makes no sense otherwise. \n",
    "- Weights will be on different scales if features not normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9240df57",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# 'Lambda' is the standard variable for the strength of the\n",
    "# regularization (as in the above formulas), but since lambda\n",
    "# is a key word in Python, these sklearn regularization tools\n",
    "# use 'alpha' instead.\n",
    "rr = Ridge(alpha=100, random_state=42)\n",
    "\n",
    "rr.fit(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4897904a",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rr.score(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39bc1c8a",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lr1.score(X_train_df, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96142a7a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let' s cross validate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcaf4315",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rr_cv_results = cross_validate(\n",
    "                X=X_train_processed, \n",
    "                y=y_train,\n",
    "                estimator=rr, \n",
    "                cv=10,\n",
    "                scoring=('r2', 'neg_mean_squared_error'),\n",
    "                return_train_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3097e9d0",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Get $R^2$ on train folds of cross validation trials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f7d719",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rr_cv_results['train_r2']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9891b0b",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Get $R^2$ on validation folds of cross validation trials:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b16b428",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rr_cv_results['test_r2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c018ea9b",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rr_cv_results['test_r2'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0502ae6",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "rr_cv_results['test_r2'].std(ddof = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f668e8c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_rrtest = np.sqrt(np.abs(rr_cv_results['test_neg_mean_squared_error']))\n",
    "RMSE_rrtest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6145010",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_rrtest.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4796d90",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "RMSE_rrtest.std(ddof = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77155102",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "L2 regularized polynomial model:\n",
    "- A little bit worse than my basic linear model.\n",
    "- Much much better than the un-regularized polynomial model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d059e368",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Could I do better? Possibly.\n",
    "\n",
    "- Tune hyperparameter $\\lambda$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b0fe796",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Tuning hyperparameters\n",
    "\n",
    "- Don't know what $\\lambda$ will allow model to perform best on validation sets.\n",
    "- Need to tune this."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "310bb513",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Adjust model stiffness/regularization parameter\n",
    "- Assess model performance in validation testing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76ca4348",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Optimizing the Regularization Hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "134e0dd0",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### The most basic hyperparameter tuning method: Make a loop!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bcbb9b4",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The regularization strength could sensibly be any nonnegative number, so there's no way to check \"all possible\" values. It's often useful to try several values that are different orders of magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc79f54",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "alphas = [1e-3, 1e-2, 1e-1, 1, 10, 100, 1e3, 1e4]\n",
    "cv_scores = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    rr = Ridge(alpha=alpha, random_state=42)\n",
    "    cv_loop_results = cross_validate(\n",
    "                X=X_train_processed, \n",
    "                y=y_train,\n",
    "                estimator=rr, \n",
    "                cv=10,\n",
    "                scoring=('neg_mean_squared_error'))\n",
    "    cv_scores.append(np.mean(np.sqrt(np.abs(cv_loop_results['test_score']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0ad821",
   "metadata": {
    "cell_style": "center",
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "sns.lineplot(x = np.log10(alphas), y = cv_scores, marker = 's', ax = ax)\n",
    "ax.set_xlabel('Log(lambda)')\n",
    "ax.set_ylabel('Mean RMSE')\n",
    "ax.set_title('RMSE averaged on validation folds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c176c935",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Could fine tune:\n",
    "- But of hyperparameter values tried $\\lambda = 100$ is best."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24158bcb",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now we *finally* report results on the true test set:\n",
    "- We have not fit optimized on it.\n",
    "- Have not tuned hyperparameters to see how well it performs on validation folds.\n",
    "\n",
    "**Test/hold-out set is our true final gold standard**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aba9d2b6",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "rr = Ridge(alpha = 100, random_state = 42)\n",
    "rr.fit(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ea6a91",
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = rr.predict(X_test_processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fac083",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ridge_RMSE_holdout = np.sqrt(mean_squared_error(y_pred, y_test))\n",
    "ridge_RMSE_holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7444db",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Given the scale of the penguin body mass (g): this is good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95174519",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "sns.histplot(y);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09604ad8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Takeaways of what we just did"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7030d2",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- Polynomial model: poor prediction performance.\n",
    "- L2 regularized the polynomial regression model (Ridge regression) + tuning\n",
    "- **Much** better test performance than unregularized polynomial model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36b7f5b",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "But:   \n",
    "- Our simple linear model with no polynomial worked well.\n",
    "- Almost as well as polynomial features\n",
    "- Think carefully before adding model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb74a08",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "A simple model with less number of good quality predictive features may work as well if not better."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad4e40f",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Logical extension:\n",
    "- Features that are correlated but don't want to throw them away.\n",
    "- L2 regularized linear model + tuning:\n",
    "    - don't throw out features.\n",
    "    - get better test performance than OLS by reducing weight variance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b8d5e7d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Sometimes though: throwing away features might work better:\n",
    "- Learn good features only with high predictive power\n",
    "- Chuck the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f92a67c",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### L1 Regularization (LASSO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2940b159",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "$$ L = |\\textbf{y} - X \\textbf{w}|_2^2 + \\lambda |\\textbf{w}|_1 $$\n",
    "\n",
    "with $|\\textbf{w}|_1 = |w_1| + |w_2| + ... + |w_m|$ as sum of absolute magnitude of the feature weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c66c03",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Taxi cab vs Euclidean distance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2d12cd8",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src = \"Images/metrics.png\" width = 450/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "555609e1",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Circle in terms of L2 vs L1 distance:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0bb0240",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\"Circle\" in L1:\n",
    "<img src = \"Images/taxcabgeometry.jpg\" width = 400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75cb0914",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\"Circles\" for different metrics\n",
    "\n",
    "<img src = \"Images/circles.png\" width = 400/>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c03c0a",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Why use the L1 magnitude $|\\textbf{w}|_1$ for regularization?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cd08c7c",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "LASSO encourages model weight sparsity: \n",
    "- prefers to drive weights $w_i$ for features with little predictive power to 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "743c89e8",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src = \"Images/different_metric_regularization.png\" width = 600 />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc13e5e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Perform LASSO regression with scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b05e734",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9719204",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Find the best LASSO model: tune regularization hyperparameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b9aa65",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "alphas = [1, 10, 100, 1e3, 1e4]\n",
    "cv_lasso_scores = []\n",
    "\n",
    "for alpha in alphas:\n",
    "    lasso = Lasso(alpha=alpha, random_state=42, max_iter = 10000)\n",
    "    cv_loop_results = cross_validate(\n",
    "                X=X_train_processed, \n",
    "                y=y_train,\n",
    "                estimator=lasso, \n",
    "                cv=10,\n",
    "                scoring=('neg_mean_squared_error'))\n",
    "    cv_lasso_scores.append(np.mean(np.sqrt(np.abs(cv_loop_results['test_score']))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ac7411",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "alphas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b8dab90",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "cv_lasso_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e4d7a9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The best LASSO model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342e20bc",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lasso_opt = Lasso(alpha=10, random_state=42,  max_iter = 100000)\n",
    "lasso_opt.fit(X_train_processed, y_train)\n",
    "\n",
    "y_pred = lasso_opt.predict(X_test_processed) # get final test prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f131c4a9",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "lasso_RMSE = np.sqrt(mean_squared_error(y_pred, y_test))\n",
    "lasso_RMSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8832e94a",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ridge_RMSE_holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d961f1e9",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Comparable between ridge and LASSO. LASSO tends to have higher weight variance than ridge.\n",
    "\n",
    "But what's the real difference?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18b019d5",
   "metadata": {
    "cell_style": "center",
    "scrolled": false,
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "print(rr.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9f96d93",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "lasso_opt.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "286eca2d",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The Strategy Behind Ridge / Lasso / Elastic Net"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbee2e1",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Overfit models overestimate the relevance that predictors have for a target. Thus overfit models tend to have **overly large coefficients**. \n",
    "\n",
    "Generally, overfitting models come from a result of high model variance. High model variance can be caused by:\n",
    "\n",
    "- having irrelevant or too many predictors\n",
    "- multicollinearity\n",
    "- large coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16e39e3",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Ridge \n",
    "When we introduce many features that:\n",
    "- we believe may all have some predictive power.\n",
    "- want to heavily penalize weight variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fbc2f51",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### LASSO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b47f3a",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We have dataset with many highly correlated features:\n",
    "- believe many are not actually adding to predictive power.\n",
    "- willing to cut away marginally unimportant features.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab46955",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Which is better:\n",
    "- depends on dataset\n",
    "- modeling goal "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70fb6962",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LEVEL UP - Elastic Net!\n",
    "Naturally, the Elastic Net has the same interface through sklearn as the other regularization tools! The only difference is that we now have to specify how much of each regularization term we want. The name of the parameter for this (represented by $\\rho$ above) in sklearn is `l1_ratio`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ddf857",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "enet = ElasticNet(alpha=10, l1_ratio=0.1, random_state=42)\n",
    "enet.fit(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ee0ea1",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "enet.score(X_train_processed, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d94b4e7",
   "metadata": {
    "cell_style": "split",
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "enet.score(X_test_processed, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2ce0cc",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Setting the `l1_ratio` to 1 is equivalent to the lasso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a65c50",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "ratios = np.linspace(0.01, 1, 100)\n",
    "preds = []\n",
    "\n",
    "for ratio in ratios:\n",
    "    enet = ElasticNet(alpha=100, l1_ratio=ratio, random_state=42)\n",
    "    enet.fit(X_train_processed, y_train)\n",
    "    preds.append(enet.predict(X_test_processed[0].reshape(1, -1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9335e2",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "lasso = Lasso(alpha=100, random_state=42)\n",
    "lasso.fit(X_train_processed, y_train)\n",
    "lasso_pred = lasso.predict(X_test_processed[0].reshape(1, -1))\n",
    "\n",
    "ax.plot(ratios, preds, label='elastic net')\n",
    "ax.scatter(1, lasso_pred, c='k', s=70, label='lasso')\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab58565",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Note on `ElasticNet()`\n",
    "Is an Elastic Net with `l1_ratio` set to 0 equivalent to the ridge? In theory yes. But in practice no. It looks like the `ElasticNet()` predictions on the first test data point as `l1_ratio` shrinks are tending toward some value around 3400. Let's check to see what prediction `Ridge()` gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d527066",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "ridge = Ridge(alpha=10, random_state=42)\n",
    "ridge.fit(X_train_processed, y_train)\n",
    "ridge.predict(X_test_processed[0].reshape(1, -1))[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2c1571",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "If you check the docstring for the `ElasticNet()` class you will see:\n",
    "- that the function being minimized is slightly different from what we saw above; and\n",
    "- that the results are unreliable when `l1_ratio` $\\leq 0.01$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517a3725",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**Exercise**: Visualize the difference in this case between `ElasticNet(l1_ratio=0.01)` and `Ridge()` by making a scatterplot of each model's predicted values for the first ten points in `X_test_processed`. Use `alpha=10` for each model.\n",
    "\n",
    "        Level Up: Make a second scatterplot that compares the predictions on the same data\n",
    "        points between ElasticNet(l1_ratio=1) and Lasso()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6d8a1c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a6c7e2bd",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<details>\n",
    "    <summary> Answer\n",
    "    </summary>\n",
    "    \n",
    "```python\n",
    "fig, ax = plt.subplots()\n",
    "enet_r = ElasticNet(alpha=10, l1_ratio=0.01, random_state=42)\n",
    "enet_r.fit(X_train_processed, y_train)\n",
    "preds_enr = enet_r.predict(X_test_processed[:10])\n",
    "preds_ridge = ridge.predict(X_test_processed[:10])\n",
    "ax.scatter(np.arange(10), preds_enr)\n",
    "ax.scatter(np.arange(10), preds_ridge);\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02c2484d",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<details>\n",
    "    <summary>\n",
    "        Level Up Answer\n",
    "    </summary>\n",
    "\n",
    "```python\n",
    "fig, ax = plt.subplots()\n",
    "enet_l = ElasticNet(alpha=10, l1_ratio=1, random_state=42)\n",
    "enet_l.fit(X_train_processed, y_train)\n",
    "preds_enl = enet_l.predict(X_test_processed[:10])\n",
    "preds_lasso = lasso.predict(X_test_processed[:10])\n",
    "ax.scatter(np.arange(10), preds_enl)\n",
    "ax.scatter(np.arange(10), preds_lasso);\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7eab0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "54323b69",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Fitting Regularized Models with Cross-Validation\n",
    "Our friend `sklearn` also includes tools that fit regularized regressions *with cross-validation*: `LassoCV`, `RidgeCV`, and `ElasticNetCV`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cc49486",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Exercise**: Use `RidgeCV` to fit a seven-fold cross-validated ridge regression model to our `X_train_processed` data and then calculate $R^2$ and the RMSE (root-mean-squared error) on our test set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeb275f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f245e82e",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<details>\n",
    "    <summary>\n",
    "        Answer\n",
    "    </summary>\n",
    "    \n",
    "```python\n",
    "from sklearn.linear_model import RidgeCV\n",
    "    \n",
    "rcv = RidgeCV(cv=7)\n",
    "rcv.fit(X_train_processed, y_train)\n",
    "rcv.score(X_test_processed, y_test)\n",
    "np.sqrt(mean_squared_error(y_test, rcv.predict(X_test_processed))\n",
    "```\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3cbb760",
   "metadata": {},
   "source": [
    "## Extra Credit: Beyond the $R^2$ Score\n",
    "\n",
    "There are other metrics! \n",
    "\n",
    "#### Mean Absolute Error (MAE)\n",
    "\n",
    "$$\\text{MAE}(y, y_\\text{pred}) = \\frac{1}{n} \\sum_{i=0}^{n} \\left| y_i - y_\\text{pred}i \\right|$$\n",
    "\n",
    "- Measures the average magnitude of errors regardless of direction, by calculating the total absolute value of errors and dividing by the number of samples (number of predictions made)\n",
    "- **This error term is in the same units as the target!**\n",
    "\n",
    "#### Mean Squared Error (MSE)\n",
    "\n",
    "$$\\text{MSE}(y, y_\\text{pred}) = \\frac{1}{n} \\sum_{i=0}^{n} (y_i - y_\\text{pred}i)^2$$\n",
    "\n",
    "- Measures the average squared error, by calculating the sum of squared errors for all predictions then dividing by the number of samples (number of predictions)\n",
    "- In other words - this is the Residual Sum of Squares (RSS) divided by the number of predictions!\n",
    "- This error term is **NOT** in the same units as the target!\n",
    "\n",
    "#### Root Mean Squared Error (RMSE)\n",
    "\n",
    "$$\\text{RMSE}(y, y_\\text{pred}) = \\sqrt{\\frac{1}{n} \\sum_{i=0}^{n} (y_i - y_\\text{pred}i)^2}$$\n",
    "\n",
    "- Measures the square root of the average squared error, by calculating the sum of squared errors for all predictions then dividing by the number of samples (number of predictions), then taking the square root of all that\n",
    "- **This error term is in the same units as the target!**\n",
    "\n",
    "Note - before, we were _maximizing_ R2 (best fit = largest R2 score). But we'd want to minimize these other error metrics.\n",
    "\n",
    "Documentation: \n",
    "- [Regression Metrics in sklearn](https://scikit-learn.org/stable/modules/classes.html#regression-metrics)\n",
    "- [User Guide for Regression Metrics in sklearn](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57990e3a",
   "metadata": {},
   "source": [
    "Note that I said that MAE and RMSE are both in the same units as our target, but you'll see that they are different here. What's the difference?\n",
    "\n",
    "> \"Taking the square root of the average squared errors has some interesting implications for RMSE. Since the errors are squared before they are averaged, the RMSE gives a relatively high weight to large errors. This means the RMSE should be more useful when large errors are particularly undesirable.\"\n",
    "\n",
    "-- Source: [\"MAE and RMSE — Which Metric is Better?\"](https://medium.com/human-in-a-machine-world/mae-and-rmse-which-metric-is-better-e60ac3bde13d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689ae2c9",
   "metadata": {},
   "source": [
    "How can we interpret these?\n",
    "\n",
    "- R2: \"Our model accounts for 61.2% of the variance in our target\"\n",
    "- MAE/RMSE: \"Our model's predictions are, on average, about __ off from our actual target values\" "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0e7efa",
   "metadata": {},
   "source": [
    "## Level Up Exercise: Name that Model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9477a3e2",
   "metadata": {},
   "source": [
    "Consider the following scenarios and describe them according to bias and variance. There are four possibilities:\n",
    "\n",
    "- a. The model has low bias and high variance.\n",
    "- b. The model has high bias and low variance.\n",
    "- c. The model has both low bias and low variance.\n",
    "- d. The model has both high bias and high variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47dec21b",
   "metadata": {},
   "source": [
    "**Scenario 1**: The model has a low RMSE on training and a low RMSE on test.\n",
    "<details>\n",
    "    <summary> Answer\n",
    "    </summary>\n",
    "    c. The model has both low bias and low variance.\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed70ef07",
   "metadata": {},
   "source": [
    "**Scenario 2**: The model has a high $R^2$ on the training set, but a low $R^2$ on the test.\n",
    "<details>\n",
    "    <summary> Answer\n",
    "    </summary>\n",
    "    a. The model has low bias and high variance.\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40a4cfe2",
   "metadata": {},
   "source": [
    "**Scenario 3**: The model performs well on data it is fit on and well on data it has not seen.\n",
    "<details>\n",
    "    <summary> Answer\n",
    "    </summary>\n",
    "    c. The model has both low bias and low variance.\n",
    "    </details>\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f37b6b",
   "metadata": {},
   "source": [
    "**Scenario 4**: The model has a low $R^2$ on training but high on the test set.\n",
    "<details>\n",
    "    <summary> Answer\n",
    "    </summary>\n",
    "    d. The model has both high bias and high variance.\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab094b00",
   "metadata": {},
   "source": [
    "**Scenario 5**: The model leaves out many of the meaningful predictors, but is consistent across samples.\n",
    "<details>\n",
    "    <summary> Answer\n",
    "    </summary>\n",
    "    b. The model has high bias and low variance.\n",
    "    </details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15aef57c",
   "metadata": {},
   "source": [
    "**Scenario 6**: The model is highly sensitive to random noise in the training set.\n",
    "<details>\n",
    "    <summary> Answer\n",
    "    </summary>\n",
    "    a. The model has low bias and high variance.\n",
    "    </details>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dplearn",
   "language": "python",
   "name": "dplearn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
